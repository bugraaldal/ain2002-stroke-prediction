{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling & Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from feature_engine.encoding import MeanEncoder\n",
    "from feature_engine.encoding import WoEEncoder\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import average_precision_score\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSVs\n",
    "real_world_df = pd.read_csv('./data/real-world-data/healthcare-dataset-stroke-data.csv')\n",
    "train_synthetic_df = pd.read_csv('./data/synthetic-data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>work_type</th>\n",
       "      <th>Residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>79.53</td>\n",
       "      <td>31.1</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Rural</td>\n",
       "      <td>78.44</td>\n",
       "      <td>23.9</td>\n",
       "      <td>formerly smoked</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Rural</td>\n",
       "      <td>103.00</td>\n",
       "      <td>40.3</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>64.87</td>\n",
       "      <td>28.8</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Private</td>\n",
       "      <td>Rural</td>\n",
       "      <td>73.36</td>\n",
       "      <td>28.8</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  gender   age  hypertension  heart_disease ever_married work_type   \n",
       "0   0    Male  28.0             0              0          Yes   Private  \\\n",
       "1   1    Male  33.0             0              0          Yes   Private   \n",
       "2   2  Female  42.0             0              0          Yes   Private   \n",
       "3   3    Male  56.0             0              0          Yes   Private   \n",
       "4   4  Female  24.0             0              0           No   Private   \n",
       "\n",
       "  Residence_type  avg_glucose_level   bmi   smoking_status  stroke  \n",
       "0          Urban              79.53  31.1     never smoked       0  \n",
       "1          Rural              78.44  23.9  formerly smoked       0  \n",
       "2          Rural             103.00  40.3          Unknown       0  \n",
       "3          Urban              64.87  28.8     never smoked       0  \n",
       "4          Rural              73.36  28.8     never smoked       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([train_synthetic_df, real_world_df], ignore_index=True, axis=0)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "categorical = [\"gender\", \"ever_married\", \"work_type\", \"Residence_type\", \"smoking_status\"]\n",
    "binary = [\"hypertension\", \"heart_disease\"] # Basically, categorical values with only 2 values. 1 or 0\n",
    "continous_numerical = [\"age\", \"avg_glucose_level\", \"bmi\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Decision Tree to predict the missing BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after decision tree regressor:  0\n"
     ]
    }
   ],
   "source": [
    "DT_bmi_pipe = Pipeline( steps=[ \n",
    "                               ('scale',StandardScaler()),\n",
    "                               ('lr',DecisionTreeRegressor(random_state=42))\n",
    "                              ])\n",
    "dt_df = df.copy()\n",
    "X = dt_df[['age','gender','bmi']].copy()\n",
    "X.gender = X.gender.replace({'Male':0,'Female':1,'Other':-1}).astype(np.uint8)\n",
    "\n",
    "missing = X[X.bmi.isna()]\n",
    "X = X[~X.bmi.isna()]\n",
    "Y = X.pop('bmi')\n",
    "DT_bmi_pipe.fit(X,Y)\n",
    "predicted_bmi = pd.Series(DT_bmi_pipe.predict(missing[['age','gender']]),index=missing.index)\n",
    "dt_df.loc[missing.index,'bmi'] = predicted_bmi\n",
    "\n",
    "print('Missing values after decision tree regressor: ',sum(dt_df.isnull().sum()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Simple Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputing:  0\n"
     ]
    }
   ],
   "source": [
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "imputed_df = df.copy()\n",
    "imputed_df['bmi'] = mean_imputer.fit_transform(imputed_df['bmi'].values.reshape(-1,1))\n",
    "print('Missing values after imputing: ',sum(imputed_df.isnull().sum()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def baseline(train_df, cv, oof_preds, target, train_auc, val_auc, pipelines, model, encoder, scaler):\n",
    "    # cast categorical features as categorical type\n",
    "    train_df[categorical] = (train_df[categorical].astype('category'))\n",
    "    for fold, (tr_ix, vl_ix) in enumerate(cv.split(train_df, target)):\n",
    "        X_train, Y_train = train_df.iloc[tr_ix], target.iloc[tr_ix]\n",
    "        X_val, Y_val = train_df.iloc[vl_ix], target.iloc[vl_ix]\n",
    "        \n",
    "        X_train = X_train.copy()\n",
    "        X_val = X_val.copy()\n",
    "        \n",
    "\n",
    "        \n",
    "        X_train = encoder.fit_transform(X_train)\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        \n",
    "        X_val = encoder.transform(X_val)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        print('_'*50)\n",
    "        \n",
    "        model.fit(X_train, Y_train)\n",
    "        \n",
    "        oof_preds.iloc[vl_ix] = model.predict_proba(X_val)[:, 1]\n",
    "        train_auc.append(roc_auc_score(Y_train, model.predict_proba(X_train)[:, 1]))\n",
    "        val_auc.append(roc_auc_score(Y_val, model.predict_proba(X_val)[:, 1]))\n",
    "        pipelines.append([encoder, scaler, model])\n",
    "\n",
    "        print(f'Val AUC: {val_auc[-1]}')\n",
    "    return pipelines, train_auc, val_auc, oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(shuffle=True, random_state=42)\n",
    "features = categorical + binary + continous_numerical\n",
    "model = LogisticRegression()\n",
    "encoder = OneHotEncoder(drop_last=True, variables=categorical)\n",
    "scaler = SklearnTransformerWrapper(StandardScaler(), variables=binary+continous_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "Val AUC: 0.8822341202238988\n",
      "__________________________________________________\n",
      "Val AUC: 0.8836537681512128\n",
      "__________________________________________________\n",
      "Val AUC: 0.8949701839417312\n",
      "__________________________________________________\n",
      "Val AUC: 0.8765518573981933\n",
      "__________________________________________________\n",
      "Val AUC: 0.8813662479306652\n",
      "\n",
      "Synthetic Train AUC: 0.8864338487351759\n",
      "Synthetic Val AUC: 0.8837552355291403\n",
      "Desicion Tree OOF AUC score: 0.8835476685117956\n"
     ]
    }
   ],
   "source": [
    "train_auc = []\n",
    "val_auc = []\n",
    "pipelines_synth = []\n",
    "oof_preds = pd.Series(0, index=train_synthetic_df.index)\n",
    "\n",
    "train_df = train_synthetic_df[features].copy()\n",
    "target = train_synthetic_df['stroke']\n",
    "\n",
    "pipelines_synth, train_auc, val_auc, oof_preds_synth = baseline(train_df, cv, oof_preds, target, train_auc, val_auc, pipelines_synth, model, encoder, scaler)\n",
    "print()\n",
    "print(f'Synthetic Train AUC: {np.mean(train_auc)}')\n",
    "print(f'Synthetic Val AUC: {np.mean(val_auc)}')\n",
    "print(f\"Desicion Tree OOF AUC score: {roc_auc_score(target, oof_preds_synth)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "Val AUC: 0.8723808665099937\n",
      "__________________________________________________\n",
      "Val AUC: 0.8739892849663773\n",
      "__________________________________________________\n",
      "Val AUC: 0.8699696930916537\n",
      "__________________________________________________\n",
      "Val AUC: 0.8829947263517521\n",
      "__________________________________________________\n",
      "Val AUC: 0.8683290276032211\n",
      "\n",
      "Imputed with mean Train AUC: 0.8753984950971601\n",
      "Imputed with Val AUC: 0.8735327197045996\n",
      "Desicion Tree OOF AUC score: 0.8735038634522456\n"
     ]
    }
   ],
   "source": [
    "train_auc = []\n",
    "val_auc = []\n",
    "pipelines_imputed = []\n",
    "oof_preds = pd.Series(0, index=imputed_df.index)\n",
    "\n",
    "imputed_df_train = imputed_df[features].copy()\n",
    "target = imputed_df['stroke']\n",
    "\n",
    "pipelines_imputed, train_auc, val_auc, oof_preds_imputed = baseline(imputed_df_train, cv, oof_preds, target, train_auc, val_auc, pipelines_imputed, model, encoder, scaler)\n",
    "print()\n",
    "print(f'Imputed with mean Train AUC: {np.mean(train_auc)}')\n",
    "print(f'Imputed with Val AUC: {np.mean(val_auc)}')\n",
    "print(f\"Desicion Tree OOF AUC score: {roc_auc_score(target, oof_preds_imputed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "Val AUC: 0.8724084975684668\n",
      "__________________________________________________\n",
      "Val AUC: 0.8739471112455498\n",
      "__________________________________________________\n",
      "Val AUC: 0.8700874886567234\n",
      "__________________________________________________\n",
      "Val AUC: 0.8831277970151672\n",
      "__________________________________________________\n",
      "Val AUC: 0.8684861285667738\n",
      "\n",
      "Desicion Tree Train AUC: 0.875469115744238\n",
      "Desicion Tree Val AUC: 0.8736114046105362\n",
      "Desicion Tree OOF AUC score: 0.8735876007847949\n"
     ]
    }
   ],
   "source": [
    "train_auc = []\n",
    "val_auc = []\n",
    "pipelines_dt = []\n",
    "oof_preds = pd.Series(0, index=dt_df.index)\n",
    "\n",
    "dt_df_train = dt_df[features].copy()\n",
    "target = dt_df['stroke']\n",
    "\n",
    "pipelines_dt, train_auc, val_auc, oof_preds_dt = baseline(dt_df_train, cv, oof_preds, target, train_auc, val_auc, pipelines_dt, model, encoder, scaler)\n",
    "print()\n",
    "print(f'Desicion Tree Train AUC: {np.mean(train_auc)}')\n",
    "print(f'Desicion Tree Val AUC: {np.mean(val_auc)}')\n",
    "print(f\"Desicion Tree OOF AUC score: {roc_auc_score(target, oof_preds_dt)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our synthetic dataframe seems to be performing the best among these dataframes. Eventhough it has way less data points.\n",
    "We are going to try other encoders from feature-engine to see if we can improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/synthetic-data/test.csv', index_col=\"id\")\n",
    "preds = pd.Series(0, index=test_df.index)\n",
    "X_test = test_df[features].copy()\n",
    "\n",
    "for pipeline in pipelines_synth:\n",
    "    X_test = test_df[features].copy()\n",
    "    encoder, scaler, model = pipeline\n",
    "    X_test = encoder.transform(X_test)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    preds += model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "preds /= len(pipelines_synth)\n",
    "preds.rename('stroke', inplace=True)\n",
    "preds.to_csv('submission1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/synthetic-data/test.csv', index_col=\"id\")\n",
    "preds = pd.Series(0, index=test_df.index)\n",
    "X_test = test_df[features].copy()\n",
    "\n",
    "for pipeline in pipelines_dt:\n",
    "    X_test = test_df[features].copy()\n",
    "    encoder, scaler, model = pipeline\n",
    "    X_test = encoder.transform(X_test)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    preds += model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "preds /= len(pipelines_dt)\n",
    "preds.rename('stroke', inplace=True)\n",
    "preds.to_csv('submission2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/synthetic-data/test.csv', index_col=\"id\")\n",
    "preds = pd.Series(0, index=test_df.index)\n",
    "X_test = test_df[features].copy()\n",
    "\n",
    "for pipeline in pipelines_imputed:\n",
    "    X_test = test_df[features].copy()\n",
    "    encoder, scaler, model = pipeline\n",
    "    X_test = encoder.transform(X_test)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    preds += model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "preds /= len(pipelines_imputed)\n",
    "preds.rename('stroke', inplace=True)\n",
    "preds.to_csv('submission3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "Val AUC: 0.9900629988133187\n",
      "__________________________________________________\n",
      "Val AUC: 0.9876649137910976\n",
      "__________________________________________________\n",
      "Val AUC: 0.9369037508434743\n",
      "__________________________________________________\n",
      "Val AUC: 0.9607340293507598\n",
      "__________________________________________________\n",
      "Val AUC: 0.9121921984825211\n",
      "\n",
      "Imputed with mean Train AUC: 0.9586418238587173\n",
      "Imputed with Val AUC: 0.9575115782562342\n"
     ]
    }
   ],
   "source": [
    "encoder = CountFrequencyEncoder(variables=categorical)\n",
    "train_auc = []\n",
    "val_auc = []\n",
    "pipelines = []\n",
    "oof_preds = pd.Series(0, index=imputed_df.index)\n",
    "target = imputed_df['stroke']\n",
    "pipelines, train_auc, val_auc, oof_preds_imputed = baseline(imputed_df, cv, oof_preds, target, train_auc, val_auc, pipelines, model, encoder, scaler)\n",
    "print()\n",
    "print(f'Imputed with mean Train AUC: {np.mean(train_auc)}')\n",
    "print(f'Imputed with Val AUC: {np.mean(val_auc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WoEEncoder.fit() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m oof_preds \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(\u001b[39m0\u001b[39m, index\u001b[39m=\u001b[39mimputed_df\u001b[39m.\u001b[39mindex)\n\u001b[0;32m      6\u001b[0m target \u001b[39m=\u001b[39m imputed_df[\u001b[39m'\u001b[39m\u001b[39mstroke\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m pipelines, train_auc, val_auc, oof_preds_imputed \u001b[39m=\u001b[39m baseline(imputed_df, cv, oof_preds, target, train_auc, val_auc, pipelines, model, encoder, scaler)\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mImputed with mean Train AUC: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(train_auc)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\ain2002-stroke-prediction\\.venv\\Lib\\site-packages\\sklearn\\utils\\_testing.py:188\u001b[0m, in \u001b[0;36m_IgnoreWarnings.__call__.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[0;32m    187\u001b[0m     warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategory)\n\u001b[1;32m--> 188\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m, in \u001b[0;36mbaseline\u001b[1;34m(train_df, cv, oof_preds, target, train_auc, val_auc, pipelines, model, encoder, scaler)\u001b[0m\n\u001b[0;32m      9\u001b[0m X_train \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     10\u001b[0m X_val \u001b[39m=\u001b[39m X_val\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> 14\u001b[0m X_train \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mfit_transform(X_train)\n\u001b[0;32m     15\u001b[0m X_train \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(X_train)\n\u001b[0;32m     17\u001b[0m X_val \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39mtransform(X_val)\n",
      "File \u001b[1;32md:\\ain2002-stroke-prediction\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32md:\\ain2002-stroke-prediction\\.venv\\Lib\\site-packages\\sklearn\\base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    877\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    879\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    881\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "\u001b[1;31mTypeError\u001b[0m: WoEEncoder.fit() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "encoder = WoEEncoder(variables=categorical)\n",
    "train_auc = []\n",
    "val_auc = []\n",
    "pipelines = []\n",
    "oof_preds = pd.Series(0, index=imputed_df.index)\n",
    "target = imputed_df['stroke']\n",
    "pipelines, train_auc, val_auc, oof_preds_imputed = baseline(imputed_df, cv, oof_preds, target, train_auc, val_auc, pipelines, model, encoder, scaler)\n",
    "print()\n",
    "print(f'Imputed with mean Train AUC: {np.mean(train_auc)}')\n",
    "print(f'Imputed with Val AUC: {np.mean(val_auc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MeanEncoder.fit() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m oof_preds \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(\u001b[39m0\u001b[39m, index\u001b[39m=\u001b[39mimputed_df\u001b[39m.\u001b[39mindex)\n\u001b[0;32m      6\u001b[0m target \u001b[39m=\u001b[39m imputed_df[\u001b[39m'\u001b[39m\u001b[39mstroke\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m pipelines, train_auc, val_auc, oof_preds_imputed \u001b[39m=\u001b[39m baseline(imputed_df, cv, oof_preds, target, train_auc, val_auc, pipelines, model, encoder, scaler)\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mImputed with mean Train AUC: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(train_auc)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\ain2002-stroke-prediction\\.venv\\Lib\\site-packages\\sklearn\\utils\\_testing.py:188\u001b[0m, in \u001b[0;36m_IgnoreWarnings.__call__.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[0;32m    187\u001b[0m     warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategory)\n\u001b[1;32m--> 188\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[1;32mIn[53], line 14\u001b[0m, in \u001b[0;36mbaseline\u001b[1;34m(train_df, cv, oof_preds, target, train_auc, val_auc, pipelines, model, encoder, scaler)\u001b[0m\n\u001b[0;32m      9\u001b[0m X_train \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     10\u001b[0m X_val \u001b[39m=\u001b[39m X_val\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> 14\u001b[0m X_train \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mfit_transform(X_train)\n\u001b[0;32m     15\u001b[0m X_train \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(X_train)\n\u001b[0;32m     17\u001b[0m X_val \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39mtransform(X_val)\n",
      "File \u001b[1;32md:\\ain2002-stroke-prediction\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32md:\\ain2002-stroke-prediction\\.venv\\Lib\\site-packages\\sklearn\\base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    877\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    879\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    881\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "\u001b[1;31mTypeError\u001b[0m: MeanEncoder.fit() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "encoder = MeanEncoder(variables=categorical)\n",
    "train_auc = []\n",
    "val_auc = []\n",
    "pipelines = []\n",
    "oof_preds = pd.Series(0, index=imputed_df.index)\n",
    "target = imputed_df['stroke']\n",
    "pipelines, train_auc, val_auc, oof_preds_imputed = baseline(imputed_df, cv, oof_preds, target, train_auc, val_auc, pipelines, model, encoder, scaler)\n",
    "print()\n",
    "print(f'Imputed with mean Train AUC: {np.mean(train_auc)}')\n",
    "print(f'Imputed with Val AUC: {np.mean(val_auc)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kolmogorov–Smirnov test is a nonparametric goodness-of-fit test and is used to determine wether two distributions differ, or whether an underlying probability distribution differes from a hypothesized distribution. It is used when we have two samples coming from two populations that can be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KstestResult(statistic=0.9120434332352834, pvalue=0.0, statistic_location=0.10622474607800855, statistic_sign=1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_2samp(oof_preds_dt[target==0], oof_preds_dt[target==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KstestResult(statistic=0.9297365330640721, pvalue=0.0, statistic_location=0.07480233752247004, statistic_sign=1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_2samp(oof_preds_imputed[target==0], oof_preds_imputed[target==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KstestResult(statistic=0.6111572201592976, pvalue=6.144495479016284e-218, statistic_location=0.042823373877734515, statistic_sign=1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_2samp(oof_preds_synth[target==0], oof_preds_synth[target==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.Series(0, index=test_df.index)\n",
    "X_test = test_df[all_features].copy()\n",
    "\n",
    "for pipeline in pipelines:\n",
    "    X_test = test_df[all_features].copy()\n",
    "    encoder, scaler, model = pipeline\n",
    "    X_test = encoder.transform(X_test)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    preds += model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "preds /= len(pipelines)\n",
    "preds.rename('stroke', inplace=True)\n",
    "preds.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
